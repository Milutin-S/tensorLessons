{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported modules.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "print(\"Imported modules.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "# Shuffle the examples\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize values: Z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized the values.\n"
     ]
    }
   ],
   "source": [
    "train_df_mean = train_df.mean()\n",
    "train_df_std = train_df.std()\n",
    "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
    "\n",
    "test_df_mean = test_df.mean()\n",
    "test_df_std = test_df.std()\n",
    "test_df_norm = (test_df - test_df_mean)/test_df_std\n",
    "\n",
    "print(\"Normalized the values.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent data\n",
    "Create a feature layer containing three features:\n",
    "- latitude X longitude\n",
    "- meadian_income\n",
    "- population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "resolution_in_Zs = 0.3 # 3/10 of a standard deviation\n",
    "\n",
    "# Create a bucket feature column for latitude.\n",
    "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
    "latitude_boundaries = list(np.arange(int(min(train_df_norm['latitude'])),\n",
    "                                     int(max(train_df_norm['latitude'])),\n",
    "                                     resolution_in_Zs))\n",
    "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n",
    "\n",
    "# Create a bucket feature column for longitude.\n",
    "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
    "longitude_boundaries = list(np.arange(int(min(train_df_norm['longitude'])),\n",
    "                                     int(max(train_df_norm['longitude'])),\n",
    "                                     resolution_in_Zs))\n",
    "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, longitude_boundaries)\n",
    "\n",
    "# Create a feature cross of latitude and longitude.\n",
    "latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\n",
    "crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n",
    "feature_columns.append(crossed_feature)\n",
    "\n",
    "# Represent median_income as floating-point value.\n",
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "feature_columns.append(median_income)\n",
    "\n",
    "# Represent population as floating-point value.\n",
    "population = tf.feature_column.numeric_column(\"population\")\n",
    "feature_columns.append(population)\n",
    "\n",
    "# Convert the list of feature columns into layer that will later be fed into the model\n",
    "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the plottiong function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_loss_curve_function.\n"
     ]
    }
   ],
   "source": [
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.95, mse.max()*1.03])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Defined the plot_loss_curve_function.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a linear regression model as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "def create_model(my_learning_rate, feature_layer):\n",
    "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(feature_layer)\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
    "    model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, label_name):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # Split the dataset into features and label.\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(label_name))\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                        epochs=epochs, shuffle=True)\n",
    "    \n",
    "    # Get details that will be useful for plotting the loss curve.\n",
    "    epochs= history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    rmse = hist[\"mean_squared_error\"]\n",
    "\n",
    "    return epochs, rmse\n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fd1d7371a990379a28203bebafbc67e68666423e9a038914ec6151938a890aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
